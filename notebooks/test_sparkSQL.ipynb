{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "spark_home = os.getenv(\"SPARK_HOME\")\n",
    "print(spark_home)\n",
    "findspark.init(spark_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bb49a7a1-18ab-4c2d-9b2b-769f6b3c2dfb;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 378ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bb49a7a1-18ab-4c2d-9b2b-769f6b3c2dfb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/7ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('persistencia').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"./data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.parallelize([1,2])\n",
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/sales_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\"). \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    load(\"data/sales_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "nuevaFila = [\n",
    "    Row(\"APPL\",\"La empresa de la manzanita\"),\n",
    "    Row(\"GOOG\",\"Líderes en cloud\"),\n",
    "    Row(\"FB\",\"Especialistas en redes sociales\"),\n",
    "    Row(\"MSFT\",\"Gran empresa\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+\n",
      "|_1  |_2                             |\n",
      "+----+-------------------------------+\n",
      "|APPL|La empresa de la manzanita     |\n",
      "|GOOG|Líderes en cloud               |\n",
      "|FB  |Especialistas en redes sociales|\n",
      "|MSFT|Gran empresa                   |\n",
      "+----+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(nuevaFila).toDF().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDescRaw = spark.sparkContext.parallelize(nuevaFila).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDesc = dfDescRaw.select(dfDescRaw[0].alias('Company2'),dfDescRaw[1].alias('Description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJoined = df.join(dfDesc,(df['Company'] == dfDesc['Company2']),\"inner\").drop(dfDesc['Company2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-------------------------------+\n",
      "|Company|Person |Sales|Description                    |\n",
      "+-------+-------+-----+-------------------------------+\n",
      "|APPL   | Chris |350.0|La empresa de la manzanita     |\n",
      "|APPL   |Mike   |750.0|La empresa de la manzanita     |\n",
      "|APPL   |Linda  |130.0|La empresa de la manzanita     |\n",
      "|APPL   |John   |250.0|La empresa de la manzanita     |\n",
      "|GOOG   |Frank  |340.0|Líderes en cloud               |\n",
      "|GOOG   |Charlie|120.0|Líderes en cloud               |\n",
      "|GOOG   |Sam    |200.0|Líderes en cloud               |\n",
      "|FB     |Sarah  |350.0|Especialistas en redes sociales|\n",
      "|FB     |Carl   |870.0|Especialistas en redes sociales|\n",
      "|MSFT   |Vanessa|243.0|Gran empresa                   |\n",
      "|MSFT   |Amy    |124.0|Gran empresa                   |\n",
      "|MSFT   |Tina   |600.0|Gran empresa                   |\n",
      "+-------+-------+-----+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfJoined.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sales_result\n"
     ]
    }
   ],
   "source": [
    "print(ruta+\"/sales_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./data/sales_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJoined.write.format(\"csv\") \\\n",
    "    .save(ruta + \"/sales_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJoined.write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(ruta + \"/sales_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-aeb8f15c-701b-4ac7-a1ab-1524d6fc3518-c000.csv\n",
      "part-00002-aeb8f15c-701b-4ac7-a1ab-1524d6fc3518-c000.csv\n",
      "part-00005-aeb8f15c-701b-4ac7-a1ab-1524d6fc3518-c000.csv\n",
      "part-00008-aeb8f15c-701b-4ac7-a1ab-1524d6fc3518-c000.csv\n",
      "part-00011-aeb8f15c-701b-4ac7-a1ab-1524d6fc3518-c000.csv\n",
      "_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "ls data/sales_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer ahora la copia guardada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSalesDesc = spark.read.format(\"csv\"). \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    load(ruta + \"/sales_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+--------------------------+\n",
      "|APPL|  Chris|350.0|La empresa de la manzanita|\n",
      "+----+-------+-----+--------------------------+\n",
      "|APPL|   Mike|750.0|      La empresa de la ...|\n",
      "|APPL|  Linda|130.0|      La empresa de la ...|\n",
      "|APPL|   John|250.0|      La empresa de la ...|\n",
      "|GOOG|Charlie|120.0|          Líderes en cloud|\n",
      "|GOOG|    Sam|200.0|          Líderes en cloud|\n",
      "|  FB|   Carl|870.0|      Especialistas en ...|\n",
      "|MSFT|    Amy|124.0|              Gran empresa|\n",
      "|MSFT|   Tina|600.0|              Gran empresa|\n",
      "+----+-------+-----+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSalesDesc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSalesDesc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modo append\n",
    "dfJoined.write.format(\"csv\") \\\n",
    "    .option(\"header\",\"True\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(ruta + \"/sales_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSalesDesc = spark.read.format(\"csv\"). \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    load(ruta + \"/sales_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSalesDesc.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formato Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------------+\n",
      "|Company| Person|Sales|         Description|\n",
      "+-------+-------+-----+--------------------+\n",
      "|   APPL|  Chris|350.0|La empresa de la ...|\n",
      "|   APPL|   Mike|750.0|La empresa de la ...|\n",
      "|   APPL|  Linda|130.0|La empresa de la ...|\n",
      "|   APPL|   John|250.0|La empresa de la ...|\n",
      "|   GOOG|  Frank|340.0|    Líderes en cloud|\n",
      "|   GOOG|Charlie|120.0|    Líderes en cloud|\n",
      "|   GOOG|    Sam|200.0|    Líderes en cloud|\n",
      "|     FB|  Sarah|350.0|Especialistas en ...|\n",
      "|     FB|   Carl|870.0|Especialistas en ...|\n",
      "|   MSFT|Vanessa|243.0|        Gran empresa|\n",
      "|   MSFT|    Amy|124.0|        Gran empresa|\n",
      "|   MSFT|   Tina|600.0|        Gran empresa|\n",
      "+-------+-------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfJoined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJoined.write.format('parquet').mode('overwrite') \\\n",
    "    .save(ruta + \"/sales_result_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromParquet = spark.read.format('parquet') \\\n",
    "    .load(ruta + \"/sales_result_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------------+\n",
      "|Company| Person|Sales|         Description|\n",
      "+-------+-------+-----+--------------------+\n",
      "|   APPL|  Chris|350.0|La empresa de la ...|\n",
      "|   APPL|   Mike|750.0|La empresa de la ...|\n",
      "|   APPL|  Linda|130.0|La empresa de la ...|\n",
      "|   APPL|   John|250.0|La empresa de la ...|\n",
      "|     FB|  Sarah|350.0|Especialistas en ...|\n",
      "|     FB|   Carl|870.0|Especialistas en ...|\n",
      "|   GOOG|  Frank|340.0|    Líderes en cloud|\n",
      "|   GOOG|Charlie|120.0|    Líderes en cloud|\n",
      "|   GOOG|    Sam|200.0|    Líderes en cloud|\n",
      "|   MSFT|Vanessa|243.0|        Gran empresa|\n",
      "|   MSFT|    Amy|124.0|        Gran empresa|\n",
      "|   MSFT|   Tina|600.0|        Gran empresa|\n",
      "+-------+-------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromParquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJoined.write.format('parquet').mode('overwrite') \\\n",
    "    .option(\"compression\", \"uncompressed\") \\\n",
    "    .save(ruta + \"/sales_result_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromParquet = spark.read.format('parquet') \\\n",
    "    .load(ruta + \"/sales_result_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------------+\n",
      "|Company| Person|Sales|         Description|\n",
      "+-------+-------+-----+--------------------+\n",
      "|   APPL|  Chris|350.0|La empresa de la ...|\n",
      "|   APPL|   Mike|750.0|La empresa de la ...|\n",
      "|   APPL|  Linda|130.0|La empresa de la ...|\n",
      "|   APPL|   John|250.0|La empresa de la ...|\n",
      "|     FB|  Sarah|350.0|Especialistas en ...|\n",
      "|     FB|   Carl|870.0|Especialistas en ...|\n",
      "|   GOOG|  Frank|340.0|    Líderes en cloud|\n",
      "|   GOOG|Charlie|120.0|    Líderes en cloud|\n",
      "|   GOOG|    Sam|200.0|    Líderes en cloud|\n",
      "|   MSFT|Vanessa|243.0|        Gran empresa|\n",
      "|   MSFT|    Amy|124.0|        Gran empresa|\n",
      "|   MSFT|   Tina|600.0|        Gran empresa|\n",
      "+-------+-------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromParquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escritura de datos en paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------------+\n",
      "|Company| Person|Sales|         Description|\n",
      "+-------+-------+-----+--------------------+\n",
      "|   APPL|  Chris|350.0|La empresa de la ...|\n",
      "|   APPL|   Mike|750.0|La empresa de la ...|\n",
      "|   APPL|  Linda|130.0|La empresa de la ...|\n",
      "|   APPL|   John|250.0|La empresa de la ...|\n",
      "|   APPL|   Mike|750.0|La empresa de la ...|\n",
      "|   APPL|  Linda|130.0|La empresa de la ...|\n",
      "|   APPL|   John|250.0|La empresa de la ...|\n",
      "|   GOOG|  Frank|340.0|    Líderes en cloud|\n",
      "|   GOOG|Charlie|120.0|    Líderes en cloud|\n",
      "|   GOOG|    Sam|200.0|    Líderes en cloud|\n",
      "|     FB|  Sarah|350.0|Especialistas en ...|\n",
      "|     FB|   Carl|870.0|Especialistas en ...|\n",
      "|   MSFT|Vanessa|243.0|        Gran empresa|\n",
      "|   MSFT|    Amy|124.0|        Gran empresa|\n",
      "|   MSFT|   Tina|600.0|        Gran empresa|\n",
      "|   GOOG|Charlie|120.0|    Líderes en cloud|\n",
      "|   GOOG|    Sam|200.0|    Líderes en cloud|\n",
      "|     FB|   Carl|870.0|Especialistas en ...|\n",
      "|   MSFT|    Amy|124.0|        Gran empresa|\n",
      "|   MSFT|   Tina|600.0|        Gran empresa|\n",
      "+-------+-------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSalesDesc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition. \n",
    "\n",
    "    El número de archivos de datos escritos es dependiente del número de particiones que el DF tiene en el momento de ejecutar la escritura. Por defecto, se escribe un fichero por partición. Para comprobar esto, podemos probar a alterar el número de particiones con el método repartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJoined.repartition(3) \\\n",
    "    .write.format(\"csv\") \\\n",
    "    .option(\"header\",\"True\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(ruta + \"/sales_result_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-8d85437f-55f3-4f68-97dc-b5b9365b7afd-c000.csv\n",
      "part-00001-8d85437f-55f3-4f68-97dc-b5b9365b7afd-c000.csv\n",
      "part-00002-8d85437f-55f3-4f68-97dc-b5b9365b7afd-c000.csv\n",
      "_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "ls data/sales_result_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "    Permite crear particiones de los datos para decidir como se almacenan los datos. El uso de particionamiento optimiza la lectura/escritura de datos: cuando queremos leer datos solo para una partición (filtro por partición), tamaño de archivos, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------------+\n",
      "|Company| Person|Sales|         Description|\n",
      "+-------+-------+-----+--------------------+\n",
      "|   APPL|  Chris|350.0|La empresa de la ...|\n",
      "|   APPL|   Mike|750.0|La empresa de la ...|\n",
      "|   APPL|  Linda|130.0|La empresa de la ...|\n",
      "|   APPL|   John|250.0|La empresa de la ...|\n",
      "|   GOOG|  Frank|340.0|    Líderes en cloud|\n",
      "|   GOOG|Charlie|120.0|    Líderes en cloud|\n",
      "|   GOOG|    Sam|200.0|    Líderes en cloud|\n",
      "|     FB|  Sarah|350.0|Especialistas en ...|\n",
      "|     FB|   Carl|870.0|Especialistas en ...|\n",
      "|   MSFT|Vanessa|243.0|        Gran empresa|\n",
      "|   MSFT|    Amy|124.0|        Gran empresa|\n",
      "|   MSFT|   Tina|600.0|        Gran empresa|\n",
      "+-------+-------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfJoined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJoined.write.format(\"csv\") \\\n",
    "    .option(\"header\",\"True\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"Company\") \\\n",
    "    .save(ruta + \"/sales_part_company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sales_part_company/:\n",
      "\u001b[0m\u001b[01;34m'Company=APPL'\u001b[0m/  \u001b[01;34m'Company=FB'\u001b[0m/  \u001b[01;34m'Company=GOOG'\u001b[0m/  \u001b[01;34m'Company=MSFT'\u001b[0m/   _SUCCESS\n",
      "\n",
      "'data/sales_part_company/Company=APPL':\n",
      "part-00002-991d82b3-c77f-4a66-8208-c9ac5d522a5d.c000.csv\n",
      "\n",
      "'data/sales_part_company/Company=FB':\n",
      "part-00008-991d82b3-c77f-4a66-8208-c9ac5d522a5d.c000.csv\n",
      "\n",
      "'data/sales_part_company/Company=GOOG':\n",
      "part-00005-991d82b3-c77f-4a66-8208-c9ac5d522a5d.c000.csv\n",
      "\n",
      "'data/sales_part_company/Company=MSFT':\n",
      "part-00011-991d82b3-c77f-4a66-8208-c9ac5d522a5d.c000.csv\n"
     ]
    }
   ],
   "source": [
    "ls -R data/sales_part_company/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
